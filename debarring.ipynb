{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic app\n",
    "letter = \"\"\"Respected sir,\n",
    "I am down with a fever and flu because of this I will not be able to come to the office for at least 5 days. As per my family doctor, it is best that I take rest and recover properly before resuming work. I have asked Sauradip to check on my clients and will try to periodically check my email if you need anything urgent. Please grant me leave for the aforementioned period. If you need additional information, please let me know. Yours Sincerely, Rajesh\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respected sir\n",
      "I am down with a fever and flu because of this I will not be able to come to the office for at least 5 days As per my family doctor it is best that I take rest and recover properly before resuming work I have asked Sauradip to check on my clients and will try to periodically check my email if you need anything urgent Please grant me leave for the aforementioned period If you need additional information please let me know Yours Sincerely Rajesh\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuations from letter\n",
    "import string\n",
    "new_letter = letter.translate(str.maketrans('', '', string.punctuation))\n",
    "print(new_letter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Respected',\n",
       " 'sir',\n",
       " 'I',\n",
       " 'am',\n",
       " 'down',\n",
       " 'with',\n",
       " 'a',\n",
       " 'fever',\n",
       " 'and',\n",
       " 'flu',\n",
       " 'because',\n",
       " 'of',\n",
       " 'this',\n",
       " 'I',\n",
       " 'will',\n",
       " 'not',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'come',\n",
       " 'to',\n",
       " 'the',\n",
       " 'office',\n",
       " 'for',\n",
       " 'at',\n",
       " 'least',\n",
       " '5',\n",
       " 'days',\n",
       " 'As',\n",
       " 'per',\n",
       " 'my',\n",
       " 'family',\n",
       " 'doctor',\n",
       " 'it',\n",
       " 'is',\n",
       " 'best',\n",
       " 'that',\n",
       " 'I',\n",
       " 'take',\n",
       " 'rest',\n",
       " 'and',\n",
       " 'recover',\n",
       " 'properly',\n",
       " 'before',\n",
       " 'resuming',\n",
       " 'work',\n",
       " 'I',\n",
       " 'have',\n",
       " 'asked',\n",
       " 'Sauradip',\n",
       " 'to',\n",
       " 'check',\n",
       " 'on',\n",
       " 'my',\n",
       " 'clients',\n",
       " 'and',\n",
       " 'will',\n",
       " 'try',\n",
       " 'to',\n",
       " 'periodically',\n",
       " 'check',\n",
       " 'my',\n",
       " 'email',\n",
       " 'if',\n",
       " 'you',\n",
       " 'need',\n",
       " 'anything',\n",
       " 'urgent',\n",
       " 'Please',\n",
       " 'grant',\n",
       " 'me',\n",
       " 'leave',\n",
       " 'for',\n",
       " 'the',\n",
       " 'aforementioned',\n",
       " 'period',\n",
       " 'If',\n",
       " 'you',\n",
       " 'need',\n",
       " 'additional',\n",
       " 'information',\n",
       " 'please',\n",
       " 'let',\n",
       " 'me',\n",
       " 'know',\n",
       " 'Yours',\n",
       " 'Sincerely',\n",
       " 'Rajesh']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "letter_token = word_tokenize(new_letter)\n",
    "letter_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Segregating nouns and adjectives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Respected', 'VBN'),\n",
       " ('sir', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('down', 'RP'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('fever', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('flu', 'NN'),\n",
       " ('because', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('I', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('come', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('office', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('at', 'IN'),\n",
       " ('least', 'JJS'),\n",
       " ('5', 'CD'),\n",
       " ('days', 'NNS'),\n",
       " ('As', 'IN'),\n",
       " ('per', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('family', 'NN'),\n",
       " ('doctor', 'NN'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('best', 'RBS'),\n",
       " ('that', 'IN'),\n",
       " ('I', 'PRP'),\n",
       " ('take', 'VBP'),\n",
       " ('rest', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('recover', 'VB'),\n",
       " ('properly', 'RB'),\n",
       " ('before', 'IN'),\n",
       " ('resuming', 'VBG'),\n",
       " ('work', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('asked', 'VBN'),\n",
       " ('Sauradip', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('check', 'VB'),\n",
       " ('on', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('clients', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('will', 'MD'),\n",
       " ('try', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('periodically', 'RB'),\n",
       " ('check', 'VB'),\n",
       " ('my', 'PRP$'),\n",
       " ('email', 'NN'),\n",
       " ('if', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('anything', 'NN'),\n",
       " ('urgent', 'JJ'),\n",
       " ('Please', 'NNP'),\n",
       " ('grant', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('leave', 'VB'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('aforementioned', 'JJ'),\n",
       " ('period', 'NN'),\n",
       " ('If', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('additional', 'JJ'),\n",
       " ('information', 'NN'),\n",
       " ('please', 'NN'),\n",
       " ('let', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('know', 'VB'),\n",
       " ('Yours', 'NNP'),\n",
       " ('Sincerely', 'NNP'),\n",
       " ('Rajesh', 'NNP')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "pos = pos_tag(letter_token)\n",
    "\n",
    "pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sir',\n",
       " 'fever',\n",
       " 'flu',\n",
       " 'able',\n",
       " 'office',\n",
       " 'least',\n",
       " 'days',\n",
       " 'family',\n",
       " 'doctor',\n",
       " 'rest',\n",
       " 'work',\n",
       " 'Sauradip',\n",
       " 'clients',\n",
       " 'email',\n",
       " 'anything',\n",
       " 'urgent',\n",
       " 'Please',\n",
       " 'aforementioned',\n",
       " 'period',\n",
       " 'additional',\n",
       " 'information',\n",
       " 'please',\n",
       " 'Yours',\n",
       " 'Sincerely',\n",
       " 'Rajesh']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = []\n",
    "noun = []\n",
    "for i in range(len(pos)):\n",
    "    if pos[i][1] == 'JJ' or pos[i][1] == 'JJR' or pos[i][1] == 'JJS' or pos[i][1] == 'NN' or pos[i][1] == 'NNS' or pos[i][1] == 'NNP' or pos[i][1] == 'NNPS':\n",
    "        lst.append(pos[i][0])\n",
    "\n",
    "\n",
    "lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'sir': 1, 'fever': 1, 'flu': 1, 'able': 1, 'office': 1, 'least': 1, 'days': 1, 'family': 1, 'doctor': 1, 'rest': 1, ...})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "for word in lst:\n",
    "    fdist[word] += 1\n",
    "fdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sir', 1),\n",
       " ('fever', 1),\n",
       " ('flu', 1),\n",
       " ('able', 1),\n",
       " ('office', 1),\n",
       " ('least', 1),\n",
       " ('days', 1),\n",
       " ('family', 1),\n",
       " ('doctor', 1),\n",
       " ('rest', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10 = fdist.most_common(10)\n",
    "fdist_top10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sir', 1),\n",
       " ('fever', 1),\n",
       " ('flu', 1),\n",
       " ('able', 1),\n",
       " ('office', 1),\n",
       " ('least', 1),\n",
       " ('days', 1),\n",
       " ('family', 1),\n",
       " ('doctor', 1),\n",
       " ('rest', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fadist_top10 = fdist.most_common(10)\n",
    "fadist_top10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAPPING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of words that got mapped with +ve dictionary: 1\n",
      "no of words that got mapped with -ve dictionary: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "top10 = []\n",
    "\n",
    "count_negative = 0\n",
    "count_positive = 0\n",
    "\n",
    "flag = False\n",
    "\n",
    "\n",
    "positive_words = [\"marriage\",\n",
    "                  \"wedding\",\n",
    "                  \"nuptials\",\n",
    "                  \"family\",\n",
    "                  \"function\",\n",
    "                  \"housewarming ceremony\",\n",
    "                  \"engagement\",\n",
    "                  \"birthday\",\n",
    "                  \"anniversary\",\n",
    "                  \"grandfather\",\n",
    "                  \"home\"]\n",
    "\n",
    "negative_words = [\"death\",\n",
    "                  \"demise\",\n",
    "                  \"passing\",\n",
    "                  \"away\",\n",
    "                  \"passed \",\n",
    "                  \"away\",\n",
    "                  \"expired\",\n",
    "                  \"health\",\n",
    "                  \"issue\",\n",
    "                  \"fever\",\n",
    "                  \"injury\",\n",
    "                  \"hospital\",\n",
    "                  \"flu\",\n",
    "                  \"doctor\",\n",
    "                  \"rest\",\n",
    "                  ]\n",
    "\n",
    "# IMPLEMENTATION THROUGH FILE LISTS\n",
    "\n",
    "for tuples1 in fadist_top10:\n",
    "    top10.append(tuples1[0])\n",
    "\n",
    "for items in top10:\n",
    "    if items in positive_words:\n",
    "        count_positive += 1\n",
    "\n",
    "for items in top10:\n",
    "    if items in negative_words:\n",
    "        count_negative += 1\n",
    "\n",
    "\n",
    "# for words in positive_words:\n",
    "#         for syn in wordnet.synsets(words):\n",
    "#             for l in syn.lemmas():\n",
    "#                     positive_words.append(l.name())\n",
    "\n",
    "# IMPLEMENTAITON THROUGH FILE I/O\n",
    "\n",
    "# f = open(\"positive_words.txt\")\n",
    "# f1 = open(\"negative_words.txt\")\n",
    "\n",
    "# for word in f:\n",
    "#     for line in top10:\n",
    "#         if word[:-1] == line:\n",
    "#             print(f\"pos: {word}\")\n",
    "#             count_positive += 1\n",
    "\n",
    "\n",
    "# for word in f1:\n",
    "#     for line in top10:\n",
    "#         if word[:-1] == line:\n",
    "#             print(word)\n",
    "#             count_negative += 1\n",
    "\n",
    "print(f\"no of words that got mapped with +ve dictionary: {count_positive}\")\n",
    "print(f\"no of words that got mapped with -ve dictionary: {count_negative}\")\n",
    "\n",
    "if count_positive > count_negative:\n",
    "    flag = False\n",
    "elif count_positive < count_negative:\n",
    "    flag = True\n",
    "else:\n",
    "    print(\"the letter will go to higher authority\\n\")\n",
    "\n",
    "# f.close()\n",
    "# f1.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEBARRING SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT = DEBARRED\n"
     ]
    }
   ],
   "source": [
    "ex_percentage_of_attendance = 60\n",
    "\n",
    "if flag == True:\n",
    "    ex_percentage_of_attendance +=10\n",
    "\n",
    "if ex_percentage_of_attendance >= 75:\n",
    "    print(f\"RESULT : NOT DEBARRED\")\n",
    "else:\n",
    "    print(\"RESULT = DEBARRED\")\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "636b2e4c25e04f16a9a47cf85a3577455a08be7dec491514b39ecb21dc1975aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
